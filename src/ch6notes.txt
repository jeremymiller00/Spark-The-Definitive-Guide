Chapter 6: Working with Different Types of Data
Most documents for DataFrame are actually under Dataset
DataFrame is simply Dataset with type Row
http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package

There are some methods specifically for DataFrame
org.apache.spark.sql.functions has lots of stuff!
lit() converts a type in another language to its corresponding Spark representation
Don't use AND, just chain together multiple filter calls
This allows Catalyst to do its optimization
Gotcha: working with nulls and boolean expressions
there are nullSafe methods (usually)

Dates and Times
Spark tries to keep it simple by using two types: Dates, timestamps
Timestamps are often stored as strings, conversion needed
Use the function *to_date* to convert string to date 
Spark will not throw an error if it fails to parse a date string
It will simply return null  
to_timestamp ALWAYS requires the format to be specified
Nulls: even when schema specifies that a column does not allow nulls, Spark will not enforce This
It is merely for optimization

There are three types of complex types: struct, array, map
You can think of Struct as DataFrame within DataFrame

UDF allow you to write your own transformations
BUT use predefined if possible; they have been optimized
If you write a UDF in Python, spark creates a Python process, executes your UDF on ALL ROWS, and then returns them to the JVM
Triggers eager evaluation
Once data is serialized to Python, Spark can no longer manage the workers processes
BUT if you write your UDF in Scala, you can still use it in Python, if you register it as a SQL function and use selectExpr

